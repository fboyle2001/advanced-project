# General
Information about each technique so they can be bundled together for the literature review. Each paper has:
- Setup
	- Online/Offline
	- Task-IL/Class-IL
	- Task-based
	- Resource considerations
- Type
	- Episodic
	- Regularisation
	- Meta-learning
	- Other
- Finegrained info (e.g. type of episodic, GAN, Feature Extraction, Sampling Type)
- Datasets
- Benchmarks
- Alleged performance and a subjective confidence rating

# Papers
## GDumb
* Technique
* Online
* Class-IL
* Can be task and non-task
* Episodic
* Greedy sampling
* CIFAR-100 and ImageNet
* Accuracy
* Very good

## Architecture Matters in CL
* Bundle with other architecture papers
* Says the network layers are significant

## ACAE-REMIND
* Technique
* Online
* Class-IL
* Task-based disjoint
* Episodic
* Feature Replay
* CIFAR-100 / ImageNetSubset 
* Good

## Dataset Distillation using Neural Feature Regression
* Technique
* Offline? (Do until converged)
* Class-IL
* Task-based
* Learn a dataset of synthetic images using a model pool
* Meta-learning / episodic - bilevel optimisation
* CIFAR-100 / TinyImageNet
* Decent

## Wide Neural Networks
* Bundle with other architecture papers
* Says that width is more important than depth

## Synaptic Intelligence
* Technique
* Regularisation with per-parameter importance computed online
* Online? Hard to tell
* Task-IL
* CIFAR-100
* Old

## Kernel Continual Learning
* Technique - not sure I included this previously?
* Online
* Task-IL
* Task-based
* Episodic - learns task-specific classifiers
* CIFAR-100 / MiniImageNet

## Rainbow
* Technique
* Online
* Class-IL
* Task-based
* Episodic - uncertainty measure
* CIFAR-100 / ImageNet

## Learning without Forgetting
* Technique
* Offline
* ?? 

## Generative Feature Replay for CI-L
* Technique
* Offline
* Class-IL
* Task-based
* Episodic - Generative Feature Replay and Feature Distillation
* CIFAR-100 / ImageNet

## Memory Aware Synapses
* Technique
* Regularisation - accumulate importance meeasure for each parameter of the network
* Offline
* Task-IL ??
* Task-based
* Scenes / Flowers / Birds (unusual sets)

## Memory Replay GANs
* Episodic - trains a GAN to act as the memory
* Got to be offline?
* Unclear if task-IL? 
* Task-based
* MNIST / SVHN

## Classification Confidence
* Dynamic Architecture - catch unknown classes using thresholding 
* Offline
* Class-IL ?
* Task-based 
* CIFAR-100 / ImageNet

## Distilling Causal Effect of Data
* Class-IL
* Online / Offline depends as plugin
* Plug-in to existing methods
* Other - removes momentum effect from samples, analyses causal effect of forgetting
* Task-based
* CIFAR-100 / ImageNet

## Adversarial Shapley
* Episodic - scores memory samples according to ability to preserve latent decision boundaries
* Online
* Class-IL
* Task-based
* CIFAR-100 / ImageNet

## OCL under Extreme Memory Constraints
* Regularisation - distill knowledge using current batch and model parameters only
* Class-IL
* Online
* Task-based
* MNIST / CIFAR-10

## Learning to Prompt
* Class-IL
* Requires pre-trained model which defeats Online? 
* Episodic? 
* Task-based
* CIFAR-100

## Hindsight Anchors
* Episodic / meta-learning - optimise samples from the dataset, stores small no of samples too
* Task-IL
* Task-based
* Online
* CIFAR-100 / ImageNet

## RWalk / EWC++
* EWC++: Regularisation - KL-divergence based regularisation
* RWalk: Regularisation - Combines Fisher information with loss over parameter changes using KL-div
* Class-IL
* Task-based
* MNIST / CIFAR-100

## Insights from the Future for CL
* Other / Episodic - ghost models with generatice model of representation space (feature generation)
* Offline
* Task-based
* Not sure if Task-IL or Class-IL
* AwA2 / aP&Y / MNIST (unusual datasets)

## Brain Inspired Replay
* Episodic - replay hidden representations generated by the networks own connections
* Class-IL 
* Offline - uses pre-trained encoders
* Task-based
* CIFAR-100

## Gradient Episodic Memory
* Online
* Episodic - minimise loss subject to loss on memory buffer being less than it was previously
* Task-IL
* Task-based
* MNIST / CIFAR-100

## Rethinking the Meta-Continual Learning
* Meta / Episodic - generative feature extractor
* CIFAR-100 / Mini-ImageNet
* Class-IL
* Online
* Task-based

## Mnemonics
* Episodic - optimise exemplars via bi-level optimisation (somewhat related to meta-learning)
* Offline
* Class-IL
* Task-based
* CIFAR-100 / ImageNet

## Tiny Episodic Memories
* Episodic - max of 13 samples per class
* Task-IL
* Task-based
* Online
* CIFAR-100 / ImageNet

## Gradient based Sample Selection
* Episodic - formulate replay buffer population
* Online
* TO FINISH