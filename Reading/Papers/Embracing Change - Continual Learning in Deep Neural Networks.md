# Embracing Change - Continual Learning in Deep Neural Networks
## Skim Read
### Abstract
* Considers regularisation, modularity, memory and meta-learning techniques

### Review
* Modularity is a key part in biological evolution
	* Specialisation of the nervous system etc are important
* Complex interactions between synpatic plasticity, episodic memory, and semantic memory are yet to be fully described by neuroscience
* Meta-learning is motivated by the brain's ability to try novel solutions with limited experience
* Comparison with curriculum learning
* Forward and backwards transfer 
* Lists desired optimal behaviour of continual learning
* Discusses different approaches: regularisation, rehersal, generative replay, modularity (dynamic)
* One issue with rehersal is scalability
* Meta-learning (learning-to-learn) could be promising
* Credit assignment problem with inefficient gradient based tug of war
* Credit assignment is the tug-of-war dynamics that optimise the parameters
* Essentially suggests the problem is that a tug-of-war without all the tasks presents no doubt leads to a loss of information for the tasks that aren't being optimised
