# Brain-inspired replay for continual learning with artificial neural networks
### Abstract and Introduction
- Mechanism thought to be important for protecting memories is reactivation of neuronal activity patterns representing those memories
- ANNs can implement this as generative replays
- Scaling this idea up is complex
- Paper proposes new variant where we replay memories generated by a network's own 'context-modulated' feedback connections
- Achieves SotA performance on CIFAR-100 without storing data
- Class-IL is much harder than Task-IL
- Established CL-ML algorithms often fail in Class-IL even on simple toy examples
- Scalability raises questions about how the brain handles replays
- Aims to tackle Class-IL
- Single-headed approach (Class-IL) vs Multi-headed approach (Task-IL)
- Class-IL is much more realistic
- Suggests for Class-IL replay may be necessary
- Concerns that generative replays just shift one hard problem (catastrophic forgetting) to another hard problem (generative modelling)

### Results
- Quantify performance by using average test accuracy over all tasks/classes seen so far
- Solid lower bound is training in the standard way one class at a time to provide baseline for catastrophic forgetting 
- Solid upper bound is training on all the data at once as if all of the data was available

#### Standard GR
- Outperforms all non-replay methods in Class-IL scenario
- Performs competitively in Task-IL scenario
- Possible to drastically reduce the quality of generated samples and still retain the performance benefits
- Small amount of 'good enough' replays can help alleviate catastrophic forgetting
- Straight-forward implementation of GR did not scale well to long series of tasks (performance rapidly declined after ~15 tasks)
- For more complex domains only those that explicitly store samples were able to perform well (e.g. iCaRL), Simple GR did not perform well

### Brain-Inspired Replay
- To improve on the standard GR the authors take inspriation from the brain
- Produces significantly better results than standard GR
- Views the Hippocampus as a generative neural network and the main model as the Cortex but this ignores that the hippocampus sits atop the cortex in the brain's hierarchy
- Incrementally training high-quality generative models is also very difficult and very expensive
- Proposes merging the generator into the main model via feedback connections (Replay-through-feedback, RtF model)
- Issue with standard VAEs is the lack of control over class generation
- Context has been shown to bias what memories are replayed in the brain
- Simple but effectively way to provide context is by inhibiting randomly selected subsets of neurons in each hidden layer based on which task should be performed (this is XdG) - requires task identifiying information
- Still possible to use context gates by conditioning on internal context in the decoder
- Another improvement suggested was to replay representations of previously learned classes internally but not down to the input layer (the brain doesn't replay mental images back to the retina)
	- Assumes that only very limited changes are made to early layers in the network which aligns with human brains 
- Simulate development by using simpler, non-overlapping version of dataset before using the real one (e.g. training on CIFAR10 before CIFAR100) and then freeze some layers
- Uses distillation: instead of labelling generated data with most likely class, soft-label them with predicted probabilities of beloning to each class - should help if replay quality is lowered
- Combination synaptic intelligence with this method led to even better results
- In Class-IL it outperformed all other methods but still substanitally below upper bound
- The proposed modifications were complementary to each other in an abalation study

### Conclusion
- Results look pretty good
- One limitation is that it was only tested on image classification
- It is a supervised method
- Used pre-trained convolutational layers
- Another limitation is that it didn't really use a very comprehensive statistic to compare
- Class-IL remains unsolved 
- Aim was to see if generative models worked as replays and they do
- GR could be a more plausible method to store replays
- Missing temporal structure

### Further Action
- Published in 2020
- Shin et al. cite 20 framework for implementation of GR
- Synaptic intelligence cite 29
- Context dependent gating XdG, cite 31 - not necessarily suitable for Class-IL due to assumptions
- What are cites 36, 37, 38?
- Sequential replays cite 68
- Interesting question remains, why is GR so much more effective than regularisation?
- Combining SI and BR were complementary suggesting they both play an important but different role