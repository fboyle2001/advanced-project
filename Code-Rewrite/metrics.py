from typing import Dict, Tuple, Union

import torch
import algorithms
import matplotlib.pyplot as plt
import matplotlib.figure

def generate_accuracy_bar_chart(
    plot_title: str,
    data: Dict[Union[str, int], Dict[str, int]]
) -> matplotlib.figure.Figure:
    """
    Generates a bar chart from the accuracy data generated by :func:`evaluate_accuracy`

    Args:
        plot_title (str): Title to display at the top of the plot
        data (Dict[Union[str, int], Dict[str, int]]): Data generated from :func:`evaluate_accuracy`

    Returns:
        matplotlib.figure.Figure: The figure containing the bar chart
    """
    total_correct = 0
    total_count = 0

    xs = []
    ys = []

    for clazz in data.keys():
        clazz_data = data[clazz]

        total_correct += clazz_data["true_positives"]
        total_count += clazz_data["real_total"]

        accuracy = (clazz_data["true_positives"]  / clazz_data["real_total"]) * 100
        xs.append(clazz)
        ys.append(accuracy)

    xs.append("total")
    ys.append((total_correct / total_count) * 100)

    fig, ax = plt.subplots()
    ax.bar(xs, ys)
    ax.set_title(plot_title)
    ax.set_xlabel("Classes")
    ax.set_ylabel("Accuracy (%)")
    ax.set_ylim(0, 100)

    for i in range(len(xs)):
        ax.text(xs[i], ys[i] // 2, f"{ys[i]:.1f}", ha="center", color="white")

    return fig

def evaluate_accuracy(
    algorithm: algorithms.BaseCLAlgorithm,
    batch_size: int = 32
) -> Tuple[int, int, Dict[Union[str, int], Dict[str, int]]]:
    dataset = algorithm.dataset
    test_loader = dataset.create_evaluation_dataloader(batch_size=batch_size)

    """
    For a class x and any class y,
    true_positive: image classified as class x when it is class x
    false_positive: image classified as class x when it is class y
    false_negative: image classified as class y when it is class x
    """
    class_evaluation = {}

    for class_name in dataset.classes:
        class_evaluation[class_name] = {
            "true_positives": 0,
            "false_positive": 0,
            "false_negative": 0,
            "real_total": 0
        }

    total = 0
    total_correct = 0

    with torch.no_grad():
        for data in test_loader:
            images, ground_truth = data

            images = images.to(algorithm.device)
            ground_truth = ground_truth.to(algorithm.device)

            predicted = algorithm.classify(images)

            total += ground_truth.size(0)
            
            for i, truth_tensor in enumerate(ground_truth):
                truth = dataset.classes[truth_tensor.item()]
                prediction = dataset.classes[predicted[i].item()] # type: ignore

                class_evaluation[truth]["real_total"] += 1

                if truth == prediction:
                    total_correct += 1
                    class_evaluation[truth]["true_positives"] += 1
                else:
                    class_evaluation[truth]["false_negative"] += 1
                    class_evaluation[prediction]["false_positive"] += 1
    
    return total, total_correct, class_evaluation

