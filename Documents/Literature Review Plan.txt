§1: Related Work
    Brief Introduction
    Catastrophic Forgetting
    [About algorithmic vs architecture]
    Three Scenarios for Continual Learning
    §1.1: Algorithm Paradigms
        Brief introduction about paradigms
        Literature flaws
        §1.1.1: Regularisation
            Outline general approach
            EWC
            Memory Aware Synapses
            LwF
            Summary
        §1.1.2: Memory-Based
            Outline approaches, include synonyms
                [Exemplars, Generative, Addons]
            [Reservoir Sampling?]
            iCaRL
            GDumb
            Mnemonics
            Gradient-based Sample Selection
            Rainbow
            ASER [Adversarial Shapley]
            Batch-level experience replay with review
            Product Quantisation
            Using GANs [Scholars]
            FearNet
            ACAE-REMIND [Feature Representations]
            Maximally Interfered Retrieval
            Brain-inspired Replay
            Learning to Prompt [?]
            On Tiny Episodic Memories
            Summary
        §1.1.3: Meta-Learning
            Outline
            Learning to learn without forgetting
            MAML / C-MAML / La-MAML
            Embracing Change
            Meta-Learning Representations
            Generative vs Discriminative - Rethinking the Meta-Continual Learning
            MER
            GEM / A-GEM
            Meta-Consolidation for Continual Learning
            Summary
        §1.1.4: Other Approaches
            Outline
            Reinforced Continual Learning
            Dataset Distillation with Neural Feature Regression
            [Online Continual Learning under Extreme Memory Constraints?]
            Distilling the knowledge in a neural network
            Drift Compensation
            LUCIR
            LNIDA
            RWalk
            Summary
    §1.2: Architecture Considerations
        Wide Neural Networks Forget Less Catastrophically
        Architecture Matters in Continual Learning
    §1.3: Benchmarking
        Accuracy
        Forgetting
        Forward and Backward Transfer
        Intransigence
        Wall-clock Time
        Memory Consumption
    §1.4: Datasets
        CIFAR-10 and MNIST
        Permutated MNIST / Rotated MNIST
        CIFAR-100 / Split CIFAR-100
        ImageNet
        CORe50
        OCR Kuzushiji
    §1.5: Summary
        
            